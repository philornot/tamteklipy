[pytest]
# Pytest configuration for TamteKlipy performance tests

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers for test categorization
markers =
    slow: Slow tests (large datasets, long operations)
    benchmark: Benchmark tests using pytest-benchmark
    api: API endpoint tests
    database: Database/SQL tests
    files: File operation tests
    concurrent: Concurrent/parallel execution tests
    smoke: Quick smoke tests for CI

# Output configuration
addopts =
    # Verbose output
    -v
    # Show local variables in tracebacks
    -l
    # Show summary of all test outcomes
    -ra
    # Capture output (show print statements on failure)
    --capture=no
    # Show warnings
    -W default
    # Strict markers (fail on unknown markers)
    --strict-markers
    # Color output
    --color=yes
    # Show slowest N tests
    --durations=10
    # Coverage (optional, comment out if not needed)
    # --cov=app
    # --cov-report=html
    # --cov-report=term-missing

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/test_run.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(name)s] %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Benchmark configuration (if pytest-benchmark installed)
[tool:pytest-benchmark]
# Minimum time per test
min_time = 0.000005
# Maximum time per test
max_time = 1.0
# Minimum rounds
min_rounds = 5
# Timer precision
timer = time.perf_counter
# Disable garbage collection during benchmark
disable_gc = true
# Warmup iterations
warmup = true
warmup_iterations = 1

# Coverage configuration (if using pytest-cov)
[coverage:run]
source = app
omit =
    */tests/*
    */conftest.py
    */__pycache__/*
    */venv/*

[coverage:report]
precision = 2
show_missing = true
skip_covered = false

[coverage:html]
directory = htmlcov